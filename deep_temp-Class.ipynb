{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c2793b",
   "metadata": {},
   "source": [
    "# DeepBeerLevelz\n",
    "\n",
    "DISCLAIMER: No beer was wasted in the making of this project.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "I was enjoying drinks with a friend at my flat and were drinking a particularly exceptional lambic beer (Cantillon Brabantiae, 2018. See: https://www.cantillon.be/) and wanted to ensure to distribute it as equitatively as possible.\n",
    "\n",
    "Unfortunately I've never made the habit of buying glasses and thus have no matching glasses and just a collection of small missmatched glasses and jars. We tried our best to be equitative by timing the time of pouring with a consistent stream size rather than by judging the volume. We took this approach because we knew that evaluating volume can be deceptive, especially with nonlinear shaped glasses such as some of mine [Pechey et al, (2015). *PLoS One*. 10(12):e0144536. https://doi.org/10.1371/journal.pone.0144536]:\n",
    "\n",
    "<img src=\"wine_glasses.png\" alt=\"Glasses_wine\" style=\"width: 600px;\"/>\n",
    "\n",
    "![wine](wine_glasses.png)\n",
    "\n",
    "We further reasoned that as this is a 750ml beer bottle. If we were off by a relatively small amount, say one person received 120ml and the other 100ml. The more small glasses we had, the larger the disparity of consumption we would have if the difference between servings remained consistent. Indeed for this particular bottle, it would mean that one person would consume on average 20% more than the other. In the end we consumed two 750ml bottles, if one consumed 20% more than the other. Following the simple formula\n",
    "\n",
    "$$\n",
    "x + 1.2*x = 1500ml\n",
    "$$\n",
    "\n",
    "One can find that in the end one person drinks 681ml, while the other drinks 818ml, a full 147ml more than the other, more than a full glass!\n",
    "\n",
    "In the end we got distracted by other topics and estimated our servings, yet the question stuck with me for a couple of days. Then I wondered, could I device a computer vision model that accurately returns the volume in the glass? I thought it could be a fun regression problem, upon researching further I found that there are comparatively few computer vision deep neural network models solving regression than classification or detection problems. I am not sure if there are simply overall less such problems to solve or there is something about them that makes them more troublesome.\n",
    "\n",
    "I have a webcam, and I can train the model with accurate measurements, so let's give it a go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e5b3d",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "I do not want to spend time drawing boxes around my glasses, as I'm generating the data I want minimal anotation. Therefore I simply set up the same background for each of my glasses, reasoning that this is quite functional too, and will help the model to ignore variations in the background that do not help in determining the volume of the glass. I may in future download some pre-trained model that can detect glasses and use it to make the model more flexible to the background.\n",
    "\n",
    "I collected images of two of my glasses at different volume levels and with variations in camera angle, and minimally also in foam levels in the beer. I wanted a working model so as stated above the background was always similar though not exactly the same. For image collection, as well as for other parts of the training I adapted a code developed here (https://github.com/SouravJohar/rock-paper-scissors) to detect hand gestures for playing rock-paper-scissors as it already implemented useful components.\n",
    "\n",
    "I filled up each glass with liquid at intervals of 10ml from 0ml to 120ml which is the limit of the smaller glass.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"image_data/1g0/14.jpg\" alt=\"Glasses_wine\" style=\"width: 140px;\"/> 0ml </td>\n",
    "<td> <img src=\"image_data/2g30/14.jpg\" alt=\"Glasses_wine\" style=\"width: 140px;\"/> 30ml </td>\n",
    "<td> <img src=\"image_data/1g60/14.jpg\" alt=\"Glasses_wine\" style=\"width: 140px;\"/> 60ml </td>\n",
    "<td> <img src=\"image_data/2g90/14.jpg\" alt=\"Glasses_wine\" style=\"width: 140px;\"/> 90ml </td>\n",
    "<td> <img src=\"image_data/1g120/14.jpg\" alt=\"Glasses_wine\" style=\"width: 140px;\"/> 120ml </td>\n",
    "</tr></table>\n",
    "\n",
    "Below is the code used for data collection, originally its own code \"collect_data.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "019d9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc = '''Script to gather data images with a particular label.\n",
    "\n",
    "# Usage: python collect_data.py <label_name> <num_samples>\n",
    "\n",
    "# The script will collect <num_samples> number of images and store them\n",
    "# in its own directory.\n",
    "\n",
    "# Only the portion of the image within the box displayed\n",
    "# will be captured and stored.\n",
    "\n",
    "# Press 'a' to start/pause the image collecting process.\n",
    "# Press 'q' to quit.\n",
    "\n",
    "# '''\n",
    "\n",
    "# import cv2\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# try:\n",
    "#     label_name = sys.argv[1]        \n",
    "#     num_samples = int(sys.argv[2])\n",
    "# except:\n",
    "#     print(\"Arguments missing.\")\n",
    "#     print(desc)\n",
    "#     exit(-1)\n",
    "\n",
    "# IMG_SAVE_PATH = 'image_data'        # Where to save the images\n",
    "# IMG_CLASS_PATH = os.path.join(IMG_SAVE_PATH, label_name)\n",
    "\n",
    "# try:\n",
    "#     os.mkdir(IMG_SAVE_PATH)         # Make the directory for all data\n",
    "# except FileExistsError:             # Unless it already exists\n",
    "#     pass\n",
    "# try:\n",
    "#     os.mkdir(IMG_CLASS_PATH)        # Make the directory for this data\n",
    "# except FileExistsError:             # Unless it already exists\n",
    "#     print(\"{} directory already exists.\".format(IMG_CLASS_PATH))\n",
    "#     print(\"All images gathered will be saved along with existing items in this folder\")\n",
    "\n",
    "# cap = cv2.VideoCapture(0)           # Start camera\n",
    "# cap.set(cv2.CAP_PROP_AUTOFOCUS, 0)  # turn the autofocus off, my camera is very annoying with this\n",
    "# cap.set(28, 600)  # I fix the focus of the camera, otherwise my specifical model of webcam makes a mess\n",
    "\n",
    "# start = False\n",
    "# count = 0\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         continue\n",
    "\n",
    "#     if count == num_samples:    #If sample number is collected, then finish\n",
    "#         break\n",
    "\n",
    "#     #Make rectangle frame for image, half of the camera\n",
    "#     cv2.rectangle(frame, (320, 0), (640, 500), (255, 255, 255), 2)\n",
    "\n",
    "#     if start:\n",
    "#         roi = frame[0:500, 320:640]   # Define ROI to match drawing,\n",
    "#         save_path = os.path.join(IMG_CLASS_PATH, '{}.jpg'.format(count + 1))\n",
    "#         cv2.imwrite(save_path, roi)     # Where to write and increase counter\n",
    "#         count += 1\n",
    "\n",
    "#     font = cv2.FONT_HERSHEY_SIMPLEX     # Define font of numbers\n",
    "#     cv2.putText(frame, \"Collecting {}\".format(count),   # Writes text of collecting #\n",
    "#             (5, 50), font, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "#     cv2.imshow(\"Collecting images\", frame)\n",
    "\n",
    "#     k = cv2.waitKey(10)\n",
    "#     if k == ord('a'):       # Start if 'a'\n",
    "#         start = not start\n",
    "\n",
    "#     if k == ord('q'):       # Quit if 'q'\n",
    "#         break\n",
    "\n",
    "# print(\"\\n{} image(s) saved to {}\".format(count, IMG_CLASS_PATH))\n",
    "# cap.release()   # stop recording\n",
    "# cv2.destroyAllWindows() # close recording window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56531202",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "I realised that 10ml is so fine-grained that I could also just make a classification algorythm which would give a reasonable estimate but I still wanted to try a regression. As there are so many great resources out there, I went for the well established and lightweight model \"SqueezeNet\" [Iandola et al, (2016). arXiv:1602.07360. https://doi.org/10.48550/arXiv.1602.07360 ]. I then needed simply to remove the head and train a few final layers:\n",
    "- I performed a Dropout to take care against overfitting\n",
    "- A simple 2D Convolution to compress the data somewhat and hopefully distill the important parameters\n",
    "- Global Average Pooling, to linearise the network and also compress it further\n",
    "- A dense layer just in case a bit of processing is still required, probably not but shouldn't hurt\n",
    "- A dense layer with a softplus function for our regression. I prefered softplus over linear as I don't want my model predicting negative values. I chose softplus over \"relu\" as I found that relu does odd things to the loss function when the training data is actually zero.\n",
    "\n",
    "I did try more complex networks just for fun but there was minimal to no improvement, so unless problem becomes more complicated and the loss no longer seems adequate, there is no reason to make the model more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b44dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from squeezenet import SqueezeNet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Activation, Dropout, Convolution2D, GlobalAveragePooling2D, Dense, MaxPool2D, Conv2DTranspose\n",
    "from keras.models import Sequential\n",
    "import os\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739d6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential([\n",
    "        SqueezeNet(input_shape=(227, 227, 3), include_top=False),\n",
    "        Dropout(0.5),\n",
    "        Convolution2D(128, (1, 1), padding='valid'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(64,'relu',kernel_initializer='he_uniform'),\n",
    "        Dense(1,'softplus')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9c2ce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " squeezenet (Functional)     (None, 13, 13, 512)       722496    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 13, 13, 512)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 13, 13, 128)       65664     \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 796,481\n",
      "Trainable params: 796,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67215f6",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "I take imported data and use regular expressions on the folder title to define the training value. Originally I had trained each glass with a different model, yet I thought it would be nice to just train everything with the same model and just have the model figure it out. Conceptually I find this interesting it's conceivable that if one successfully trains with a sufficient number of glasses the model will be able to interpolate new glasses types it hasn't seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ba10b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_data/2g110\n",
      "image_data/1g50\n",
      "image_data/2g50\n",
      "image_data/1g0\n",
      "image_data/2g70\n",
      "image_data/2g10\n",
      "image_data/1g20\n",
      "image_data/2g90\n",
      "image_data/1g10\n",
      "image_data/2g120\n",
      "image_data/2g100\n",
      "image_data/1g120\n",
      "image_data/1g70\n",
      "image_data/1g110\n",
      "image_data/1g40\n",
      "image_data/1g90\n",
      "image_data/1g60\n",
      "image_data/2g30\n",
      "image_data/2g20\n",
      "image_data/2g60\n",
      "image_data/1g80\n",
      "image_data/1g100\n",
      "image_data/2g40\n",
      "image_data/2g0\n",
      "image_data/1g30\n",
      "image_data/2g80\n"
     ]
    }
   ],
   "source": [
    "dir_data = 'image_data'\n",
    "glass_Train = \"\"\n",
    "# load images from the directory\n",
    "dataset = []\n",
    "for directory in os.listdir(dir_data):\n",
    "    if glass_Train in directory:\n",
    "        path = os.path.join(dir_data, directory)\n",
    "        print(path)\n",
    "        if not os.path.isdir(path):\n",
    "            continue\n",
    "        for item in os.listdir(path):\n",
    "            # to make sure no hidden files get in our way\n",
    "            if item.startswith(\".\"):\n",
    "                continue\n",
    "            img = cv2.imread(os.path.join(path, item))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (227, 227))\n",
    "            dataset.append([img, directory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "623be9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(label):\n",
    "    return(int(re.sub(\"[1-2]g\",\"\",label)))\n",
    "\n",
    "data, labels = zip(*dataset)\n",
    "labels = list(map(mapper,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d4295",
   "metadata": {},
   "source": [
    "## Training\n",
    "The model trains relatively fast, in about 10 minutes.My laptop, though 5 years old, has an integrated NVIDIA Quadro M1200, 4 GB RAM, 128 Bit width, 5000 MHz Clock Speed GPU which integrates neatly with tensorflow's CUDA functionality and works well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ba4d50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " squeezenet (Functional)     (None, 13, 13, 512)       722496    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 13, 13, 512)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 128)       65664     \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 128)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 796,481\n",
      "Trainable params: 796,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "244/244 [==============================] - 48s 192ms/step - loss: 8.4196 - mean_absolute_error: 9.0561\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 47s 192ms/step - loss: 3.2812 - mean_absolute_error: 3.8719\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 47s 192ms/step - loss: 2.3935 - mean_absolute_error: 2.9556\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 57s 236ms/step - loss: 2.1879 - mean_absolute_error: 2.7400\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 61s 250ms/step - loss: 1.8578 - mean_absolute_error: 2.4012\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 61s 250ms/step - loss: 1.6723 - mean_absolute_error: 2.2048\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 61s 249ms/step - loss: 1.5041 - mean_absolute_error: 2.0251\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 61s 250ms/step - loss: 1.4242 - mean_absolute_error: 1.9355\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 61s 250ms/step - loss: 1.2535 - mean_absolute_error: 1.7590\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 61s 250ms/step - loss: 1.1069 - mean_absolute_error: 1.5999\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = get_model()\n",
    "print(model.summary())\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='log_cosh', # I tried MAE and it worked pretty well, I don't see too much difference with log_cosh, however\n",
    "                     # I realised interestingly that log_cosh leads to a faster reduction of loss when the values\n",
    "                     # are smaller, I think that makes sense from looking at the function shape near zero compared\n",
    "                     # to absolute\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "\n",
    "# start training\n",
    "history = model.fit(np.array(data), np.array(labels), epochs=10)\n",
    "\n",
    "# save the model for later use\n",
    "model.save(\"deepBeerLevelz.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d6bf6",
   "metadata": {},
   "source": [
    "I check the mean absolute error and explained variance on our separate validation dataset. The mean absolute error is pretty good. Absolute error is intuitive for this type of scenarios as it can relate back to physical terms, we can directly interpret that the measurement is on average off by 1.038ml (may vary), not bad at all! A relative error would be a bit more difficult to interpret as we have some values that are zero. The almost one explained variance is also a good sign that we correctly capture the dynamics of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71514f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.038 (ml)\n",
      "EV: 0.999\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"deepBeerLevelz.h5\")\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "data_pred = model.predict(np.array(data))\n",
    "print(\"MAE: \"+str(np.round(mean_absolute_error(labels,data_pred),3))+\" (ml)\")\n",
    "print(\"EV: \"+str(np.round(explained_variance_score(labels,data_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644fd05c",
   "metadata": {},
   "source": [
    "## Testing on separate datasets, and testing the regression\n",
    "The steps so far suggest that the model can return values pretty well, but so far all the data is still in steps of 10. It also doesn't seem to struggle with the fact that there are two glass types. I then create new datasets with values the model has not trained for and analyse whether it is able to correctly interpolate the values for both glass types. Also since these values aren't very small it's possible to use percentage error too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e90e5f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_datasets/1g105\n",
      "test_datasets/2g105\n",
      "test_datasets/1g95\n",
      "test_datasets/1g25\n",
      "test_datasets/2g95\n",
      "test_datasets/2g25\n"
     ]
    }
   ],
   "source": [
    "# model = load_model(\"deepBeerLevels.h5\")\n",
    "dir_test = \"test_datasets/\"\n",
    "glass_Test = \"\"\n",
    "# load images from the directory\n",
    "dataset_test = []\n",
    "for directory in os.listdir(dir_test):\n",
    "    if glass_Test in directory:\n",
    "        path = os.path.join(dir_test, directory)\n",
    "        print(path)\n",
    "        if not os.path.isdir(path):\n",
    "            continue\n",
    "        for item in os.listdir(path):\n",
    "            # to make sure no hidden files get in our way\n",
    "            if item.startswith(\".\"):\n",
    "                continue\n",
    "            img = cv2.imread(os.path.join(path, item))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (227, 227))\n",
    "            dataset_test.append([img, directory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29eb5a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.417 (ml)\n",
      "MAPE: 0.02\n",
      "EV: 0.997\n"
     ]
    }
   ],
   "source": [
    "data_test, labels_test = zip(*dataset_test)\n",
    "labels_test = list(map(mapper,labels_test))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "data_pred = model.predict(np.array(data_test))\n",
    "print(\"MAE: \"+str(np.round(mean_absolute_error(labels_test,data_pred),3))+\" (ml)\")\n",
    "print(\"MAPE: \"+str(np.round(mean_absolute_percentage_error(labels_test,data_pred),3)))\n",
    "print(\"EV: \"+str(np.round(explained_variance_score(labels_test,data_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925b498",
   "metadata": {},
   "source": [
    "While the model has slightly more error (1.417ml), it will be able to accurately predict the value within a tolerable error!\n",
    "\n",
    "All that's left now is to make a fun video of beer being poored and the model predicting the values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70400052",
   "metadata": {},
   "source": [
    "## Making evidential video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c2eebeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_sequence = \"sequence_datasets/2gsequence/\"\n",
    "dataset_sequence = []\n",
    "for item in os.listdir(dir_sequence):\n",
    "    # to make sure no hidden files get in our way\n",
    "    if item.startswith(\".\"):\n",
    "        continue\n",
    "    img = cv2.imread(os.path.join(dir_sequence, item))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (227, 227))\n",
    "    dataset_sequence.append(img)\n",
    "    \n",
    "order_sequence = np.argsort([int(re.sub(\".jpg\",\"\",im)) for im in os.listdir(path)])\n",
    "dataset_sequence = np.array(dataset_sequence)[order_sequence]\n",
    "predict_sequence = (model.predict(dataset_sequence))[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90319b2",
   "metadata": {},
   "source": [
    "One can see that the predicted level increases over time either from pooring or from foam becoming liquid. In hindsight I could have made my training datasets include foam and thus account for that, definitely for a future model. So to be accurate the model calculates the amount of liquid beer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "35a3fd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAruklEQVR4nO3deXxcZdn/8c+VvWm673tKV0pZSksXNsv2yKYgAoKIIErlQURxY9Efm6KAKz6CAqKiIIvsCi0iUHaKbWmhK6Ub3ZsuaZMmzTbX7485mU7SpJksk5nJfN+vV16ZOcuca6bJN3fvc859m7sjIiLpIyPRBYiISPtS8IuIpBkFv4hImlHwi4ikGQW/iEiaUfCLiKQZBb8kPTP7i5n9JHh8nJktb6fjupmNbI9jNZeZrTGzk+N8jMjnLh2Lgl/aRBBE5WZWamZbgtAoaOvjuPsb7j4mhnouNbM32/r4Ua8/28z2Bu93l5m9bmaHxut4Im1JwS9t6TPuXgAcCUwCflR/AzPLaveq4ueq4P32BGYDf2vrA1iYfk+lTekHStqcu28AZgLjIdJl8g0zWwGsCJadaWYLzKzYzN42s8Nq9zezCWY238xKzOwxIC9q3XQzWx/1fIiZPWVmRWa23cx+Z2YHA38ApgUt8uJg21wz+4WZfRL8r+QPZtYp6rW+b2abzGyjmV3WjPdbAzwKjIt6rQwzu87MVgZ1PW5mPaPWTw3ed7GZLTSz6VHrZpvZbWb2FlAGHHSg4x/oWGY208yuqrf9QjM7J3g81sxeMrMdZrbczM5v5Bi9zexfQb07zOwN/UFKXfqHkzZnZkOA04H3oxafDUwBxpnZBOBPwNeBXsC9wHNBMOcAzxBuPfcE/gF8vpHjZAL/AtYChcAg4FF3XwpcAbzj7gXu3j3Y5XZgNHAEMDLY/sbgtU4FvgecAowCYu4/D2q+CHg3avE3g/f8KWAgsBO4O9h+EPA88JPgPX4PeNLM+kTtfzEwA+gSvL8DafRYwCPAhVG1jgOGAc+bWWfgJeDvQF/gAuCeYJv6vgusB/oA/YAbAI33kqrcXV/6avUXsAYoBYoJB9U9QKdgnQMnRm37e+DH9fZfTji4jgc2Aha17m3gJ8Hj6cD64PE0oAjIaqCeS4E3o54bsAcYEbVsGrA6ePwn4PaodaODukc28n5nE26NFwMVwC7gpKj1S+s9HwBUAVnAtcDf6r3ei8AlUa99awyf98kxHKtL8L6HBetuA/4UPP4C8Ea9170XuCl4/Jeoz/1W4NnGPg99pdaXWvzSls529+7uPszdr3T38qh166IeDwO+G3QbFAddMUMIt1YHAhs8SJtAYy3eIcBad6+OobY+QD4wL+qYs4LlBMeNrrGpVjbA1R7+30Qn4Ezgiaguq2HA01HHWgrUEG4tDwPOq/f+jyUc2LWia2lKo8dy9xLC/7u4INj2QuDhqP2m1KvjIqB/A8f4OfAx8G8zW2Vm1zWjPkkyHelEmyS36CBfB9zm7rfV38jMPgUMMjOLCv+hwMoGXnMdMNTMshoI//rdENuAcuAQD5+DqG8T4T8ktYY2/lbqHcg9BLxhZh8D/wN8ENR2mbu/VX97M1tHuMV/+YFeNtbjH+hYgUeAm8zsdcLnS16N2u81dz+lqQMEf0C+S/gP9njgFTP7r7u/3Iw6JUmoxS+JcD9whZlNCa5a6WxmZ5hZF+AdoBq42syyg5OQkxt5nfcIB/btwWvkmdkxwbotwOCg/702nO8Hfm1mfSHc125mnw62fxy41MzGmVk+cFNz3pCZTSN8cndxsOgPwG1mNixY38fMzgrWPQR8xsw+bWaZQd3TzWxwc44Z5UDHAniBcOv+VuCx4LOA8PmR0WZ2cfBZZ5vZUcHJ8frv70wzG2lmRrhbqwYI1d9OUoOCX9qdu88FLgd+R/hE5MeE++Rx90rgnOD5DsL90E818jo1wGcIn6j9hPDJxy8Eq18hHMKbzWxbsOza4Fjvmtlu4D/AmOC1ZgK/Cfb7OPjelN9Z+KqhUsIno38UvA7AXcBzhLtGSgif+J0SHGsdcBbhE6RFhFve36flv4+NHis4XgXhz/Bkwidya5eXEP4fygWEz6tsBu4Achs4xijCn1cp4T/O97j7qw1sJynA6naliohIR6cWv4hImlHwi4ikGQW/iEiaUfCLiKSZlLiOv3fv3l5YWJjoMkREUsq8efO2uXuf+stTIvgLCwuZO3duossQEUkpZtbgHejq6hERSTMKfhGRNKPgFxFJMwp+EZE0o+AXEUkzCn4RkTSj4BcRSTMKfhGRJFNZHWL55hJu+ediqmvaftqDlLiBS0QkXazdvoc7Zy3n+Q83AXDOhMEcOrhbmx5DwS8ikmClFdXc/Nxixg/sys3/XFJnXVuHPij4RUQS7vkPNvLEvPU8Ma/u8uG9O8fleOrjFxGJs9Xb9lBcVsnFD8zhu48vBKCssppNu8oBqKje148/ZXhPAD5z+ED+9tXGpptuHbX4RUTiqLiskhN+MbvOsrMnDOSGpz9k3Y5y/n75FG58djEApx/an7sumEBNyMnLzoxbTQp+EZE42lBcvt+yix94L/L4yofnRx7/7sIjycgw4pj5gLp6RKSD2lVexegfzeTm5xa3+DXcndKK6jrLXl22lcrqpi+xDIWct1duY+32ssiyX553+H7bFZdVccq4frz83U+RkWEtrrU51OIXkQ7pP0u2UFkd4i9vr6GsspqK6hBfmDSEob3yGdwjn+uf+pBDBnblS1OHRfZxd5ZvKWFjcTknju3Hn99aw63/WsLfL58CDu+u3sFvX17BJdOGcctZ4/c75rbSCnKyMvjnwo1s2FnOPbNX1lk/cViPBmvt0yWXEX0K2vYDOAAFv4h0CDv2VFJRXcN9r69iUPdO/OT5pZF1j89dD8CzCzYCcNLYvry8bCsAX5o6jPLKGu6YtYy/vL0mss8Np4/lqfkbAPji/XPqHOuV5Vu5pd7xNxSXc8ztrzRa39vXnUifLrl1lr1w9XGsLCrl2JG9m/VeW8vcvV0P2BKTJk1yzcAlIo25Z/bH3DlreYv2PW18f15cvJlQM6NwRJ/OPHHF0fTonENRSQVH3fafRre99+KJfPqQ/kC4C2p3eRVzVu/g3ImDW1RzrMxsnrtPqr9cLX4RSai9VTX8Z+kW1m4v48LJQ8nNymDTrnJeWrKVrxxT2ODVLZ9sL2NIz068sWIb+TmZ/L5el0p9N5w+lm2llQzslrffDVIzF22OPP7ilKHc8tlDGPXDmZFl4wd1JdOMLbsr2Lx7b2T5yqI9fPbuN/ny1EJue2EpjTlyaPdI6AN065RNt07ZDOmZf8Ca40nBLyLtrqomxN/eWcvC9cWR7heAn78YbrWfMq4fLy3ZwqINu3j+w01cftxwjhvVhy//6b3GXhKAQd07Ra6iWfnT08ls4GTp3uoQ08f0obI6xIy/zouE+V8vm8zxo8Pzkl8ybRgPvrOWcyYM4ppTRkdC2t1xhztmLePe11exbkd5ndC/+qRRjOjTmaoa56MtJXzjhJHkZSffNTTq6hGRduPunH33W1RUh1i2uaRNX/ud609kQLdOfLSlhN4FufTsnBPTfnurati8ay+F9e6SdXfMGr/KZtOucqb9bF+f/q1nHcKXpxW2qPZ4UVePiCSUu1NUUsHC9bvqLM/LzmD66L6cftgArn7k/Zhea+KwHtz9xSN5aclmRvbtwuAenRjQrRMAo/t1aVZdedmZ+4U+cMDQBxjQrROLbvk0K7aE/4BNGNrwFTvJSMEvIm1m/c4yOmVn0qsgl6KSCnp1zmHJpt28s3L7fv3gJ43tywOXHlVnmbszqm8XXlqyhYfnrGVrSUWd9YW98lmzvYw/f+UouuZlc3GCW9gFuVkpFfi1FPwi0mIfby0lNyuDnp1zeOy/67j1X0vokptFXk4mRfVCu1bvgly+OHkIF0weut+6s44YBMC4gV351smjgPD5gA07y9lYXM6kwp6s3raHrnnZ8XtTaUDBL5KGyiqryc3K5Kn565l6UC8KcrPo0TmHqpoQZZU1zF6+layMDKaP6UN+TiZmRlllNZXVIbrnh/vOi8sqOflXr+332iUV1ZTUu9u11skH9+MHp45pVndMdmYGhb07R7pjxvRvXleO7E/BL5JG1mzbw9f+OpePt5but+65q47hs797a7/lPfKz2VlWFXn+2wsn8NnDB0YGFot2+XHDmTisB1c8NJ9PH9KPFxdviawb1iufP16y33lGSQBd1SOSBiqqaxjzo1lxPcYXpwzlx2eNJzPD+GR7GX275vKrlz7ivtdXcfNnxnHWEYPoEeOVNtI2dFWPSJpZuK6Y7z+xkM8ePpDSiprI8n5dc9myu+H+9wM558hB5GVn8vc5n0SW3XD6WL4waSi52Rl1brQa2is/WH8wV504Un3ySUbBL9JBXfHQPDbt2ssv/v1RneVHDOke6YKZMrwnc1bv4L6LJ9IpJ5OSvdWsKirl/KOGkJuVSUVVDZU1Ibp2yo6E93WnjeWwm/8NwGXHDCcr88A3KCn0k4+CXyTF7Sqr4oZnPuT5Dzbxv9NHMKpvAd8JZnmK9s+rjmVHWSWThvXgnZXbyciA6aP7smxzCeMGdm34xTvtH9pd87I5bXx/Zi7a3GToS3JSH79Iitq6ey+791Zx8q9eb3SbAd3yGNAtj0uOLoxcKtkWQiHHocEhESR5qI9fJIXdPnMZvQtyGNu/K4cP6UZ2ZgaTf/pyk/v99bLJjGrmnayxaK8JQyQ+FPwiSW5lUSl/eG3f6JNZ9UK3sFc+vzz/CO6ctYw5q3dw4eQh3Hb2oWwoLk/oCJCSvOLaQWdm15jZYjNbZGaPmFmemQ03szlm9rGZPWZmur5LpAHuzh/fWMUZv32jzvLqkFMdNXj8by6YwMRhPdhTGb5p6tyJQ8jIMIW+NCpuwW9mg4CrgUnuPh7IBC4A7gB+7e4jgZ3AV+NVg0gqu/f1Vfzk+aXsrQrP79q7oG4b6c5zD+OksX0ZG9zJ+tVjhwMwql/7TeEnqSnep+SzgE5mlgXkA5uAE4EngvUPAmfHuQaRlLNs825un7ks8rxzTiZzf3QKb157AgW54R7as48YxAOXHhW5fv5zEwaz5vYzdPmkNCluffzuvsHMfgF8ApQD/wbmAcXuXjuQx3qgwUsNzGwGMANg6ND9B3MS6ah+NnMp9762qs6yy48/CIDBPfJ5+/oT+WR7GTlZupRSWiZuwW9mPYCzgOFAMfAP4NRY93f3+4D7IHw5ZxxKFEk6W3bvjYT+tIN68ciMqezYU0mP/H2t+K552Ywf1C1RJUoHEM+rek4GVrt7EYCZPQUcA3Q3s6yg1T8Y2BDHGkRSxqZd5Rx9+74ZnR6ZMRUg5pmkRGIVz+D/BJhqZvmEu3pOAuYCrwLnAo8ClwDPxrEGkaS3Ztse/u+Vj1m1rZTa+ylfuPq4xBYlHVo8+/jnmNkTwHygGnifcNfN88CjZvaTYNkD8apBJBmFQs4nO8oo7N2ZmpAz/Rez66z/xXmHNz6EgkgbiOsNXO5+E3BTvcWrgMnxPK5IstlVVsXsj7by+kfbeHL++ga3+fFZhyR8KkFJD7pzV6QdfP2huby7akeD604Z14/fX3SkBjyTdqOfNJE4enPFNhauK94v9M+ZEL6K+bhRvbn/y5MU+tKu1OIXiaMvPTAn8vh/p49gd3kV15wymt4Fudx81iHkKPAlART8InFSWR2q8/z8SUMYHkwYDpqgRBJHzQ2RONm0q7zO84Hd8xJUiUhdCn6RONmwc1/w52ZlkJuVeYCtRdqPunpE4mBVUSkvLNoEwL0XT6RfV7X2JXko+EXi4MRfvgaAGZwwpq8GVJOkop9GkTY2Z9X2yOOC3CyFviQdtfhF2khxWSWvfVTElt179y3UuLKShBT8Im1gV3kVR9z6EgBnHTEQgPMmDubciYMTWZZIgxT8Ii1UWR2iJuR0ysnk1WVbI8vfWLGNQd078fPzDk9gdSKNU+ejSAtd8qf3OPjGWQDs2FMZWb5jTyWDe3RKVFkiTVLwi7TQO8FJ3E27yikuCwe/WXjdIAW/JDEFv0grfbK9jB1l4ekRewWzZQ3uruCX5KU+fpEWuPaJDyKPNxSXs7Osih75OVQE4/Pk5+pXS5KXWvwiLfDY3HWRx1t2V7BzTyU9Oufw+BXTmDK8J6eN75/A6kQOTM0SkVbaVV7FzrIqBnXPY1D3Tjz29WmJLknkgNTiF2mlp+avZ+mm3XTrlJPoUkRiouAXaSZ3xwwunDwEgK0lFQAM0rDLkiIU/CLNVB1y3GFgt7pX7nzt+IMSVJFI8yj4RZqpdmatnKwMzjhsAABThvfUjFqSMnRyV6SZooP/1+cfwTUnj2Zk34IEVyUSO7X4RZqpsmZf8OdkZSj0JeUo+EVi8ODba7j/9VXAvhZ/dqZ+fSQ1qatHJAY3PbcYgK8cU8iiDbuA8Dy6IqlIwS/SDCN/ODPyOEctfklR+skVaaEenXXDlqQmBb9IC00Z3jPRJYi0iIJfpAk1oYYnzrXawfdFUoyCX6QJtVfxiHQUCn6RJuytqkl0CSJtSlf1iADvrtrOLf9cQijkXHZsIX9+aw23njWeycN7RiZXiZafk5mAKkXaRlyD38y6A38ExgMOXAYsBx4DCoE1wPnuvjOedYg0pibknPqb11mxtTSy7NonPwTg1y99xCMzpjbY4u/TJbfdahRpa/Hu6rkLmOXuY4HDgaXAdcDL7j4KeDl4LtJm3PedjF2ycTeF1z3PtU98wF/eWl1n+bodZYy44YU6oQ9w5NDudZ7vqayOPK6dU/fcIwfHoXKR9hG3Fr+ZdQOOBy4FcPdKoNLMzgKmB5s9CMwGro1XHZJefvOfj3h2wUZuPHMcZZU1LN9SAuybKvHiaYX8c+FGvv3Yggb3f+TyqUwb0YuLH5jDGyu28ee3VjOqb5fI+hs/M45pI3rRp0Atfkld8ezqGQ4UAX82s8OBecC3gH7uvinYZjPQr6GdzWwGMANg6NChcSxTUtmt/1wCwJaSvUwY0p3f/GcFAF/5y38b3H7EDS/st+zOzx/GtBG9yMo0BgRj7Nf24d/yzyVcf9pYAP7zneMZGfVHQCRVxTP4s4AjgW+6+xwzu4t63Tru7mbW4EXS7n4fcB/ApEmTGr6QWtLaD5/+kIfnfBJ5/vwHmxrd1gy8kZ+i848ast+y/Jx9vxo/m7kMgB75ulNXOoZ49vGvB9a7+5zg+ROE/xBsMbMBAMH3rXGsQTqod1dtrxP60U4+uB+zvzeduy44AoAvTBrCyttO59KjCwF4dMZULpk2jE+N7sO/rzm+wddo6N4sBb90FHFr8bv7ZjNbZ2Zj3H05cBKwJPi6BLg9+P5svGqQjmnL7r1ccN+7AJx6SH/uvuhIXly8mSsfns+xI3tz57mH0bNzDgO7d2LZ9BIuPbqQjAzjR2cczFUnjqR3QS5TD+p1wGPU/9/BNSePJiNDd+pKxxDv6/i/CTxsZjnAKuArhP+X8biZfRVYC5wf5xokBc1bu4Mn52/gmyeOpCA3i1eXF/Gp0X1Yt6OM8+99J7LdHeceRmaGcfqhA1hz+xl1XiMnK4NrTx0beZ6VmUHvGE/KZtYL+d5d1NqXjiOuwe/uC4BJDaw6KZ7HldT3vX98wOpte/h7VHfO5OE9eW/1jsjzV783nW6d4jPP7cDudSdS76L5dKUD0ZANklTcnc/d8xart+3Zb1106AP0LohfK/yrxw7n9EP7M6JPZwC65Okmd+k4FPySVLaVVvL+J8UAZDXSp37exMF8aepQCnLjF8bdOmVzz0UT6RV0DeVlaYgG6Tia/M0xs8HABcBxwECgHFgEPA/MdHcNXShtZmNxeeRxfk4mH9z8adydj7aUMmf1di6aMmy//vd4qr0LWOd1pSM5YPCb2Z+BQcC/gDsIX3qZB4wGTgV+aGbXufvr8S5U0sOmXeX7LTMzxvTvwpj+7X/z1BmHDuC/a3YytFd+ux9bJF6aavH/0t0XNbB8EfBUcLWObquVNrG3qoalm8JDLBw3qjfXn3ZwgiuCS44u5NxJQ+LarSTS3g7409xI6EevrwQ+btOKJG2d8uvXWLejnLzsDP562eSkmOHKzBT60uE01dXzIeHhlPdbRXjEhcPiUpWkhXU7yjj1N6/zxP8ezdsrt7NuR7ibp1N2ZlKEvkhH1VRT5sx2qULS0nMLN7KnsoZrHlvAss0lkeXVjcxxKyJt44CXc7r72ugvYCdQEvUl0mK7y6sAIqF/0ZTw6aKQgl8krmLqvDSzrwO3AHvZ1/XjwEFxqkvSQP2btKaP6Uv3/GyOGdE7QRWJpIdYz1p9Dxjv7tviWYykl7Xby8jPyaQm5FRUhziqsAenjGtwegYRaUOxBv9KoCyehUjHFwo5P35+CdtLKxnVt4DlW0r4wqQh3HGurhEQaU+xBv/1wNtmNgeoqF3o7lfHpSrpkOZ9spM/v7WmzjJdvCPS/mIN/nuBV4APAQ3RIC0yf+3O/ZbtCk7wikj7iTX4s939O3GtRDq05ZtL+NnMZfTIz2Zn2b6wL62oTmBVIukp1tE5Z5rZDDMbYGY9a7/iWpl0KC8v2wLAV44ZHll28sF9uekzhySqJJG0FWuL/8Lg+/VRy3Q5p8TE3Xlv9Q565Gdz9UmjOHFsX4pKKjhhbN9ElyaSlmIKfncf3vRWIg17cfFmZi8vijwfP6hbAqsRkQN29ZjZsU2s72pm49u2JOkIQiHn24++zyvLtrCyKHyj1sBueQmuSkSg6Rb/583sTmAWMA8oIjwe/0jgBGAY8N24Vigp579rdvD8B5t4ZsFGnlmwkSunjwDgqSuPSXBlIgJND8t8TXAS9/PAecAAwjNwLQXudfc341+ipJrz/vBOnef3zF5Jr8459FeLXyQpNNnH7+47gPuDL5EWOXqkxt8RSRaabF3aVGMja55xaP92rkREGqOphaRNlewN35D1w9MP5pKjC/nXBxsZ1bcL4wd1TXBlIlKryeA3swxgqru/3Q71SIqrHYKhe342OVkZnHPk4ARXJCL1NdnV4+4h4O52qEU6gKLS8Bh+vQpyElyJiDQm1j7+l83s86aJUKUJG4rD8+YO6p6f4EpEpDGxBv/XgX8AlWa228xKzGx3HOuSFLUxCP6B3XXppkiyinXIhi7xLkQ6hmcXbASgIFfXDYgkq1jn3DXgImC4u//YzIYAA9z9vbhWJynjb++sYdbizSzdFP6PoHoFRZJXrM2yewhPwHIi8GOglPAJ36PiVJekkL++s4Ybn12c6DJEJEaxBv8Udz/SzN4HcPedZqbLNgSAlVtLE12CiDRDrCd3q8wsk/AY/JhZHzQFowRKK2rqPB/TT6eERJJZrMH/W+BpoJ+Z3Qa8Cfw0blVJStlTUV0n7Gd9+7gEViMiTYn1qp6HzWwecBJgwNnuvjSWfYP/KcwFNrj7mWY2HHgU6EV4qOeL3b2yRdVLUiitqKZzbmbkuU7siiS35gzS1hsoc/ffAduCAI/FtwgP41zrDuDX7j4S2Al8tRk1SBIqraimIC+bWd8+jqevPDrR5YhIE2IKfjO7CbiWfXPuZgMPxbDfYOAM4I/BcyN8ZdATwSYPAmc3q2JJOqUV1RTkZjK2f1cmDO2R6HJEpAmxtvg/B3wW2APg7huBWM7g/Qb4AftOBPcCit29Oni+HhjU0I5mNsPM5prZ3KKiooY2kSSxp6JaN2yJpJBYg7/S3Z19V/V0bmoHMzsT2Oru81pSmLvf5+6T3H1Snz59WvIS0k5K91bTWcEvkjJi/W193MzuBbqb2eXAZTQ9I9cxwGfN7HTC8/R2Be4KXiMraPUPBja0rHRJBu5OaWU1XRT8Iikjpha/u/+CcL/8k8AY4EZ3/78m9rne3Qe7eyFwAfCKu18EvAqcG2x2CfBsC2uXJFBWWYM7FOQp+EVSRcy/re7+EvCSmfUGtrfimNcCj5rZT4D3gQda8VqSYO9/Ugygrh6RFHLAFr+ZTTWz2Wb2lJlNMLNFwCJgi5mdGutB3H22u58ZPF7l7pPdfaS7n+fuFa17C5JIb368DYDxA7sluBIRiVVTzbTfATcA3YBXgNPc/V0zGws8AsyKc32SpMoqq8nKyGBryV4GdMvj8CHdE12SiMSoqeDPcvd/A5jZre7+LoC7L9Pdmelt3I0vAtC7IJfCXpptSySVNHVyN3ogtvJ667yNa5EUsaeiOvJ4W2kFg3t0SmA1ItJcTbX4Dw+mWDSgU9R0i0b4Ek1JQx/XG4Z5cA+1+EVSyQGD390zD7Re0tOK/YJfLX6RVNKcQdpEgPCMW9EGKfhFUoqCX5pl8669fLB+V51lhb2aHMFDRJKIgl+aZd3OsjrPv378QQzpqT5+kVSi4JdmWbs9HPx/vWwyk4b1YMbxByW4IhFpLt1nL80ya9Em+nTJZepBvTh+tEZNFUlFavFLs6zatodJw3qQk6UfHZFUpd9eiVko5KzfWa4+fZEUp+CXmBWVVlBZHWKILt8USWnq45cD+vmLy/j34i1cPG0YNz67GNCduiKpTsEvAOytquHe11Zx2JBu7K2s4aA+BVx4/7vs2FMJEAl9gEMGdk1UmSLSBhT8aaaopIJd5VV8vLWEKx6aD0BuVgYV1aEm9gx7+sqj6dtVwzSJpDIFfwdWE3IMyMgID6G9aVc5x93xKtWhugOr1ob+wG55bNy1N7L8+58ew5mHDQDgUz+fzU8/dygThvZon+JFJG4U/B2Uu/P5379NUUkF504czF0vr6izvmteFgW5WXWC/u3rT8LdMTNK9lbRJS87sm7BjafQPT+n3eoXkfhR8HdQw69/IfK4fugfN6o39148kU7Zmbz18XZWb9/DtIN6AlA7wU506AMKfZEORMHfAVVU19R5fs6EQfzkc+NZsaWUN1YUceX0kZHun2NH9ebYUb0TUaaIJIiCvwPZU1HN6x8VsWrbnsiyl645nhF9CsjIMA4f0l1z44qIgr+j2FtVwxUPzeONFdvqLB/Vr0uCKhKRZKXg7yBOv+uNOi19gPduOClB1YhIMtOQDR1ARXXNfqEP6Hp7EWmQWvwdwPLNJZHH3//0GPp2ydWwCiLSKAV/iquqCfGlP84B4PAh3fnS1GF065TdxF4iks4U/Cnu5aVb2b23GoBnrjw6ch2+iEhj1Mefwp6av54rHpoXea7QF5FYKPhT2HceXwjAkUO78871Jya4GhFJFerqSTE1Iee7jy8gI6p1/9DXppCfo39KEYmN0iLFbCut4JkFGyPPf3j6wQp9EWkWdfWkmOhe/GG98vnaccMTVouIpCYFf4qJHkp/d3mVTuiKSLPFLfjNbIiZvWpmS8xssZl9K1je08xeMrMVwXfN7NEMzr7k31lWlcBKRCRVxbPFXw18193HAVOBb5jZOOA64GV3HwW8HDyXGEW3+B+bMTVxhYhIyopb8Lv7JnefHzwuAZYCg4CzgAeDzR4Ezo5XDR1RKEj+q04YyZSDeiW4GhFJRe3Sx29mhcAEYA7Qz903Bas2A/0a2WeGmc01s7lFRUXtUWZKKezdOdEliEiKinvwm1kB8CTwbXffHb3O3R3whvZz9/vcfZK7T+rTp0+8y0wZIQ9/XDqlKyItFdfgN7NswqH/sLs/FSzeYmYDgvUDgK3xrKGjCXKfDF2PJSItFM+regx4AFjq7r+KWvUccEnw+BLg2XjV0BHta/GrzS8iLRPPWz6PAS4GPjSzBcGyG4DbgcfN7KvAWuD8ONbQ4dRe1aPL90WkpeIW/O7+Jo13RWtOwBYLJ3+Gkl9EWkg9xSlGLX4RaS0Ff4qJnNxV8otICyn4U4wu5xSR1lLwp5hI8KvFLyItpOBPMfu6ehJbh4ikLgV/ivHIyV0lv4i0jII/xXjkcs4EFyIiKUvBn2J0OaeItJaCP8Xo5K6ItJaCP8XoOn4RaS0Ff4pxXccvIq2k4E8xtZMXqMUvIi2l4E8xtVMvKvdFpKUU/ClGV/WISGsp+FOMa1hmEWklBX8bqwk5izfuoriskpK9VXyyvYzK6hClFdWU7K1iV3kVT7+/np17Kikuq+Tsu9/i/z2zKObXj9y5G6f6RaTji+cMXGll4bpiHnhzNbMWbaayJhTTPmbhIF+wrpjLjh3O8N6dm9xn35y7in4RaRkFfxu59skPWLa5hH5dczl4QFeOKuzJsws28NGWUgCmHdSL6lCInKwMRvXtwgfrixnZt4BXlxdRVFLBttKKmIJfwzKLSGsp+NvI9j2VnDKuH/d/eVJk2TdOGNnkfu+s3M6F979LdY03uS3ozl0RaT318beR6poQA7rlNXu/rMxwgNeEYgv+fdfxN/tQIiKAgr/NVIeczBakce0+1aHYzgu4Wvwi0koK/jZSXeNkZzb/48zKaGaLXxOxiEgrKfjbSE2rW/yx9vGHv5tO74pICyn420hVKBRpvTdHVkb4nyD2Fr+GbBCR1lHwt4FQyHHfF+LNkdnMrp6QhmUWkVZS8LeBquDEbO0VOs3R3OBXi19EWkvB3wZqQ7tlXT3N6+PXsMwi0loK/jZQG9qtOblbE+PlnCG1+EWklRT8baD2rtt2afHrck4RaSUFfxuojvTxt8fJ3drtlPwi0jIK/jbQuhZ/Rp3XaIpa/CLSWgr+NlDTmj7+Zo/Vo4lYRKR1FPxtoCoYf78lQzZkBgFe4zF29QTngJX7ItJSCv420KoWf3Ov4w++q8UvIi2VkOA3s1PNbLmZfWxm1yWihrZUFfTPZ7fgBq7IVT3NHI9fRKSl2j34zSwTuBs4DRgHXGhm49q7jrZUUV0DQGYLhmzIyDDMmr6Of1dZFT94YiFFJRWR/UREWiIRM3BNBj5291UAZvYocBawpK0P9MOnP2TO6h1t/bL7Kd1bDcDY/l1atH9WhvG3d9fyrw83hRd4uEvH3YPv8MmOsjr7KPZFpKUSEfyDgHVRz9cDU+pvZGYzgBkAQ4cObdGBBnbvxJh+LQvj5hreuzNDeua3aN+rTxzFss0lkTQ3whOthL+Hnx85tDuvfVTEp0b3oWfnXPp3bf5sXyIikMRz7rr7fcB9AJMmTWpRx3Ysc94mg2+eNCrRJYhIGknEyd0NwJCo54ODZSIi0g4SEfz/BUaZ2XAzywEuAJ5LQB0iImmp3bt63L3azK4CXgQygT+5++L2rkNEJF0lpI/f3V8AXkjEsUVE0p3u3BURSTMKfhGRNKPgFxFJMwp+EZE0Y54Cg36ZWRGwtoW79wa2tWE57SUV607FmkF1t7dUrDsVawYY5u596i9MieBvDTOb6+6TEl1Hc6Vi3alYM6ju9paKdadizQeirh4RkTSj4BcRSTPpEPz3JbqAFkrFulOxZlDd7S0V607FmhvV4fv4RUSkrnRo8YuISBQFv4hImunQwZ+Kk7qb2Roz+9DMFpjZ3ETX0xgz+5OZbTWzRVHLeprZS2a2IvjeI5E1NqSRum82sw3BZ77AzE5PZI31mdkQM3vVzJaY2WIz+1awPKk/7wPUneyfd56ZvWdmC4O6bwmWDzezOUGePBYMK5+SOmwffzCp+0fAKYSnd/wvcKG7t/ncvm3JzNYAk9w9qW8WMbPjgVLgr+4+Plh2J7DD3W8P/tD2cPdrE1lnfY3UfTNQ6u6/SGRtjTGzAcAAd59vZl2AecDZwKUk8ed9gLrPJ7k/bwM6u3upmWUDbwLfAr4DPOXuj5rZH4CF7v77RNbaUh25xR+Z1N3dK4HaSd2lDbj760D9mezPAh4MHj9I+Jc8qTRSd1Jz903uPj94XAIsJTx3dVJ/3geoO6l5WGnwNDv4cuBE4IlgedJ93s3RkYO/oUndk/6HjvAP2L/NbF4w4Xwq6efum4LHm4F+iSymma4ysw+CrqCk6jKJZmaFwARgDin0ederG5L88zazTDNbAGwFXgJWAsXuXh1skip50qCOHPyp6lh3PxI4DfhG0DWRcjzch5gq/Yi/B0YARwCbgF8mtJpGmFkB8CTwbXffHb0umT/vBupO+s/b3Wvc/QjCc4JPBsYmtqK21ZGDPyUndXf3DcH3rcDThH/oUsWWoF+3tn93a4LriYm7bwl+0UPA/SThZx70NT8JPOzuTwWLk/7zbqjuVPi8a7l7MfAqMA3obma1sxamRJ40piMHf8pN6m5mnYOTYJhZZ+B/gEUH3iupPAdcEjy+BHg2gbXErDY8A58jyT7z4GTjA8BSd/9V1Kqk/rwbqzsFPu8+ZtY9eNyJ8AUiSwn/ATg32CzpPu/m6LBX9QAEl4n9hn2Tut+W2IoOzMwOItzKh/B8yH9P1prN7BFgOuHharcANwHPAI8DQwkPo32+uyfVidRG6p5OuNvBgTXA16P6zhPOzI4F3gA+BELB4hsI95cn7ed9gLovJLk/78MIn7zNJNw4ftzdbw1+Px8FegLvA19y94rEVdpyHTr4RURkfx25q0dERBqg4BcRSTMKfhGRNKPgFxFJMwp+EZE0o+CXtGJmvaJGhdwcNUpkqZndE6djftvMvnyA9Wea2a3xOLZIQ3Q5p6St9hiVM7jTcz5wZNQ4L/W3sWCbY9y9LF61iNRSi18EMLPpZvav4PHNZvagmb1hZmvN7Bwzu9PC8yTMCoYhwMwmmtlrwYB6L9a7I7XWicD82tA3s6uD8ek/MLNHITLOzmzgzHZ5s5L2FPwiDRtBOLQ/CzwEvOruhwLlwBlB+P8fcK67TwT+BDR0l/UxhMehr3UdMMHdDwOuiFo+Fziuzd+FSAOymt5EJC3NdPcqM/uQ8K37s4LlHwKFwBhgPPBSuKeGTMIjTdY3gPA4L7U+AB42s2cID3FRayswsO3KF2mcgl+kYRUA7h4ysyrfdzIsRPj3xoDF7j6tidcpB/Kinp8BHA98BvihmR0adAPlBduKxJ26ekRaZjnQx8ymQXj4YTM7pIHtlgIjg20ygCHu/ipwLdANKAi2G02SjVIpHZeCX6QFguk8zwXuMLOFwALg6AY2nUm4hQ/h7qCHgu6j94HfBuO9A5wAPB/PmkVq6XJOkTgzs6eBH7j7ikbW9yM8BPdJ7VuZpCsFv0icmdkYwvPjvt7I+qOAKndf0K6FSdpS8IuIpBn18YuIpBkFv4hImlHwi4ikGQW/iEiaUfCLiKSZ/w984LGX/TjZQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,999/30+0.001,1/30),predict_sequence);\n",
    "plt.title(\"Predicted Beer levels\");\n",
    "plt.xlabel(\"Time (s)\");\n",
    "plt.ylabel(\"Beer (ml)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6381466e",
   "metadata": {},
   "source": [
    "I then simply label the images and make a video using ffmpeg command line which I'm more familiar with for such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1cc40d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFont, ImageDraw\n",
    "\n",
    "dir_sequence_out = \"sequence_datasets/2gsequence_predict/\"\n",
    "try:\n",
    "    os.mkdir(dir_sequence_out)         # Make the directory for all data\n",
    "except FileExistsError:             # Unless it already exists\n",
    "    pass\n",
    "title_font = ImageFont.truetype('font/arial_narrow_7.ttf', 30)\n",
    "\n",
    "for i, item in enumerate(os.listdir(dir_sequence)):\n",
    "    # to make sure no hidden files get in our way\n",
    "    if item.startswith(\".\"):\n",
    "        continue\n",
    "    my_image = Image.open(os.path.join(dir_sequence, item))\n",
    "    title_text = \"Predicted: \"+str(np.round(predict_sequence[int(re.sub(\".jpg\",\"\",item))-1],1))+\"ml\"\n",
    "    image_editable = ImageDraw.Draw(my_image)\n",
    "    image_editable.text((15,440), title_text, (235, 235, 235), font=title_font)\n",
    "    my_image.save(dir_sequence_out + re.sub(\".jpg\",\"_marked.jpg\",item.zfill(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "392cbac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"ffmpeg -r 1 -i \"+dir_sequence_out+\"img%01d.png -vcodec mpeg4 -y 2gsequence_video.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42560e41",
   "metadata": {},
   "source": [
    "<video controls src=\"sequence_datasets/2gsequence_predict/2gsequence_video.mp4\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cc782005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"sequence_datasets/2gsequence_predict/2gsequence_video.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(dir_sequence_out+\"2gsequence_video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "eeedf526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"2gsequence_predict/2gsequence_video.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"2gsequence_predict/2gsequence_video.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77c942",
   "metadata": {},
   "source": [
    "## Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d759c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
